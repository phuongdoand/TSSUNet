{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad55fad7",
   "metadata": {},
   "source": [
    "# 1. Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8225b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fde0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "sigma70 = pd.read_csv(\"20211213.Sigma70.txt\", sep = '\\t', names = [\"name\", \"seq\", \"strand\", \"express\"])\n",
    "\n",
    "# Create label\n",
    "label = []\n",
    "for i in range(496):\n",
    "    label.append(0)\n",
    "for i in range(10):\n",
    "    label.append(1)\n",
    "for i in range(495):\n",
    "    label.append(0)\n",
    "    \n",
    "sigma70[\"labels\"] = [label] * len(sigma70)\n",
    "\n",
    "# Split independent testing dataset\n",
    "ind_sigma70 = sigma70.sample(n=200, random_state=37)\n",
    "train_sigma70 = sigma70[~sigma70.index.isin(ind_sigma70.index)]\n",
    "\n",
    "val_sigma70 = ind_sigma70.sample(n=100, random_state=11)\n",
    "test_sigma70 = ind_sigma70[~ind_sigma70.index.isin(val_sigma70.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959e326a",
   "metadata": {},
   "source": [
    "# 2. Import libraries and define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e942675b",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f52475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import Variable\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "torch.manual_seed(1)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    gpu = 0\n",
    "    \n",
    "BATCH_SIZE = 256 # Batch size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095bf1ac",
   "metadata": {},
   "source": [
    "## Define function used in the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b52d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating sliding sequences from the data\n",
    "\n",
    "def sliding(df, random_sampling=False):\n",
    "    negative = []\n",
    "    seqs = []\n",
    "    labels = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        # Negative data\n",
    "        for i in range(0, 371): # 371 sequences\n",
    "            e = i + 150\n",
    "            negative.append(row[\"seq\"][i:e])\n",
    "\n",
    "        for i in range(431, 851): # 420 sequences\n",
    "            e = i + 150\n",
    "            negative.append(row[\"seq\"][i:e])\n",
    "        \n",
    "        if random_sampling == True:\n",
    "            # Random sampling the negative data to 60 sequences\n",
    "            random.seed(idx)\n",
    "            random_negative_seq = random.sample(negative, 60)\n",
    "            for negative_seq in random_negative_seq:\n",
    "                seqs.append(negative_seq)\n",
    "\n",
    "            for i in range(len(random_negative_seq)):\n",
    "                labels.append(label[0:150])\n",
    "        elif random_sampling == False:\n",
    "            random.seed(idx)\n",
    "            random_negative_seq = random.sample(negative, 400)\n",
    "            for negative_seq in random_negative_seq:\n",
    "                seqs.append(negative_seq)\n",
    "\n",
    "            for i in range(len(random_negative_seq)):\n",
    "                labels.append(label[0:150])\n",
    "\n",
    "        # Positive data\n",
    "        for i in range(371, 431): # 60 sequences\n",
    "            e = i + 150\n",
    "            seqs.append(row[\"seq\"][i:e])\n",
    "            labels.append(row[\"labels\"][i:e])\n",
    "\n",
    "    labels_com = []\n",
    "    for lab in labels:\n",
    "        lab = [lab]\n",
    "        lab_arr = np.asarray(lab)\n",
    "        lab_arr = lab_arr.transpose(1,0)\n",
    "        labels_com.append(lab_arr)\n",
    "\n",
    "    return seqs, np.asarray(labels_com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00354ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference for stacked energy and bendability values\n",
    "\n",
    "energy_ref = {'GC': -14.59, 'GT': -10.51, 'AC': -10.51, 'GA': -9.81, 'TC': -9.81, 'CG': -9.69,\n",
    "       'CC': -8.26, 'GG': -8.26, 'AT': -6.57, 'CA': -6.57, 'TG': -6.57, 'CT': -6.78,\n",
    "       'AG': -6.78, 'TT': -5.37, 'AA': -5.37, 'TA': -3.82}\n",
    "energy = -np.array(list(energy_ref.values()))\n",
    "energy_normed = 2*(energy - np.min(energy))/(np.max(energy) - np.min(energy)) - 1\n",
    "energy_ref_normed = dict(zip(energy_ref.keys(), energy_normed))\n",
    "\n",
    "bendability_ref = {'AAT': -0.28, 'AAA': -0.274, 'CCA': -0.246, 'AAC': -0.205, 'ACT': -0.183, 'CCG': -0.136,\n",
    "    'ATC': -0.11, 'AAG': -0.081, 'CGC': -0.077, 'AGG': -0.057, 'GAA': -0.037, 'ACG': -0.033,\n",
    "    'ACC': -0.032, 'GAC': -0.013, 'CCC': -0.012, 'ACA': -0.006, 'CGA': -0.003, 'GGA': 0.013,\n",
    "    'CAA': 0.015, 'AGC': 0.017, 'GTA': 0.025, 'AGA': 0.027, 'CTC': 0.031, 'CAC': 0.04,\n",
    "    'TAA': 0.068, 'GCA': 0.076, 'CTA': 0.09, 'GCC': 0.107, 'ATG': 0.134, 'CAG': 0.175,\n",
    "    'ATA': 0.182, 'TCA': 0.194, 'ATT': -0.28, 'TTT': -0.274, 'TGG': -0.246, 'GTT': -0.205,\n",
    "    'AGT': -0.183, 'CGG': -0.136, 'GAT': -0.11, 'CTT': -0.081, 'GCG': -0.077, 'CCT': -0.057,\n",
    "    'TTC': -0.037, 'CGT': -0.033, 'GGT': -0.032, 'GTC': -0.013, 'GGG': -0.012, 'TGT': -0.006,\n",
    "    'TCG': -0.003, 'TCC': 0.013, 'TTG': 0.015, 'GCT': 0.017, 'TAC': 0.025, 'TCT': 0.027,\n",
    "    'GAG': 0.031, 'GTG': 0.04, 'TTA': 0.068, 'TGC': 0.076, 'TAG': 0.09, 'GGC': 0.107,\n",
    "    'CAT': 0.134, 'CTG': 0.175, 'TAT': 0.182, 'TGA': 0.194}\n",
    "\n",
    "bendability = np.array(list(bendability_ref.values()))\n",
    "bendability_normed = 2*(bendability - np.min(bendability))/(np.max(bendability) - np.min(bendability)) - 1\n",
    "bendability_ref_normed = dict(zip(bendability_ref.keys(), bendability_normed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc21cb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for pytorch\n",
    "\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, seqs, labels, bend_ref):\n",
    "        self.seqs = seqs\n",
    "        self.labels = labels\n",
    "        \n",
    "        self.bend_ref = bend_ref\n",
    "        self.avg_bend = sum(bend_ref.values())/len(bend_ref)\n",
    "        \n",
    "        assert len(self.labels) == len(self.seqs)\n",
    "    \n",
    "    def seq2onehot(self, seq):   \n",
    "        module = np.array([[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]])\n",
    "        promoter_onehot = []\n",
    "        for item in seq:\n",
    "            if item == 't' or item == 'T':\n",
    "                promoter_onehot.append(module[0])\n",
    "            elif item == 'a' or item == 'A':\n",
    "                promoter_onehot.append(module[1])\n",
    "            elif item == 'g' or item == 'G':\n",
    "                promoter_onehot.append(module[2])\n",
    "            elif item == 'c' or item == 'C':\n",
    "                promoter_onehot.append(module[3])\n",
    "            else:\n",
    "                promoter_onehot.append([0,0,0,0])\n",
    "\n",
    "        data = np.array(promoter_onehot)\n",
    "        data = np.float32(data)\n",
    "        data = np.transpose(data, (1,0))\n",
    "\n",
    "        return data\n",
    "    \n",
    "    def energyStacking(self, seq):\n",
    "        energy = []\n",
    "        energy.append(self.avg_energy)\n",
    "        \n",
    "        for i in range(len(seq) - 1):\n",
    "            dimer = ''.join(seq[i:i+2])\n",
    "            dimer_val = self.energy_ref[dimer]\n",
    "            #energy.append(energy[-1] + dimer_val)\n",
    "            energy.append(dimer_val)\n",
    "        \n",
    "        return np.float32(np.array(energy))\n",
    "    \n",
    "    def dnaBendability(self, seq):\n",
    "        bend = []\n",
    "        bend.append(self.avg_bend)\n",
    "        \n",
    "        for i in range(len(seq) - 2):\n",
    "            trimer = ''.join(seq[i:i+3])\n",
    "            trimer_bend = self.bend_ref[trimer]\n",
    "            bend.append(trimer_bend)\n",
    "            \n",
    "        bend.append(self.avg_bend)\n",
    "        \n",
    "        return np.float32(np.array(bend))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seqs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        seq = self.seqs[idx]\n",
    "        seq = list(itertools.chain.from_iterable(seq))\n",
    "        onehot = self.seq2onehot(seq)\n",
    "        bend = self.dnaBendability(seq)\n",
    "        seq = np.vstack([onehot, bend])\n",
    "        \n",
    "        label = self.labels[idx]\n",
    "        label = np.float32(label)\n",
    "        label = np.transpose(label, (1,0))\n",
    "        label = torch.tensor(label)\n",
    "\n",
    "        return seq, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc8fdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_metrics(tp, fp, tn, fn):\n",
    "    acc = (tp+tn)/(tp+fp+tn+fn)\n",
    "    recall = tp/(tp+fn)\n",
    "    spec = tn/(tn+fp)\n",
    "    f1 = 2*tp/(2*tp+fp+fn)\n",
    "    mcc = (tp*tn-fp*fn)/math.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n",
    "    return acc, spec, recall, f1, mcc\n",
    "\n",
    "def evaluate_model(model, dataset, dataloader, threshold=0.05):\n",
    "    recon_data = []\n",
    "    recon_conf_data = []\n",
    "    labels = []\n",
    "    with torch.no_grad():    \n",
    "        for i, (x, x_label) in enumerate(dataloader):\n",
    "            x = x.cuda()\n",
    "            recon, recon_conf = model(x)\n",
    "            if i == 0:\n",
    "                recon_data = recon.cpu().numpy()\n",
    "            else:\n",
    "                recon_data = np.vstack((recon_data, recon.cpu().numpy()))\n",
    "    \n",
    "    for i in range(len(recon_data)):\n",
    "        recon_data[i][recon_data[i] >= threshold] = 1\n",
    "        recon_data[i][recon_data[i] < threshold] = 0\n",
    "    \n",
    "    pos_list = []\n",
    "    neg_list = []\n",
    "    correct_neg = 0\n",
    "    correct_pos = 0\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        if sum(dataset[i][1][0].detach().numpy()) == 0.0:\n",
    "            neg_list.append(i)\n",
    "            if sum(recon_data[i][0]) == 0.0:\n",
    "                correct_neg += 1\n",
    "        else:\n",
    "            pos_list.append(i)\n",
    "            if sum(recon_data[i][0]) != 0.0:\n",
    "                correct_pos += 1\n",
    "                \n",
    "    fn = len(pos_list) - correct_pos\n",
    "    fp = len(neg_list) - correct_neg\n",
    "    \n",
    "    return evaluation_metrics(correct_pos, fp, correct_neg, fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f33c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focal loss used in the training process\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, alpha=.2, gamma=3., reduction=\"mean\"):\n",
    "        super(FocalLoss, self).__init__()\n",
    "#         self.alpha = torch.tensor([alpha, 1-alpha]).cuda()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, recon_x, x):\n",
    "        # Calculate BCE loss\n",
    "        BCE_loss = F.binary_cross_entropy(recon_x, x)\n",
    "        \n",
    "        # Calculate focal loss\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        # p = torch.sigmoid(recon_x)\n",
    "        # pt = p * x + (1 - p) * (1 - x)\n",
    "        # pt = recon_x * x + (1 - recon_x) * (1 - x)\n",
    "        F_loss = (1-pt)**self.gamma * BCE_loss\n",
    "    \n",
    "        if self.alpha > 0:\n",
    "            at = self.alpha * x + (1 - self.alpha) * (1 - x)\n",
    "            F_loss = at * F_loss\n",
    "        \n",
    "        if self.reduction == \"mean\":\n",
    "            F_loss = F_loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            F_loss = F_loss.sum()\n",
    "        \n",
    "        return F_loss\n",
    "\n",
    "criterion = FocalLoss()\n",
    "\n",
    "class_loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7989a8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.squeeze = nn.AdaptiveAvgPool1d(1)\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _ = x.size()\n",
    "        y = self.squeeze(x).view(b, c)\n",
    "        y = self.excitation(y).view(b, c, 1)\n",
    "        return x * y.expand_as(x)\n",
    "    \n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None, seq_length=150):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels*4\n",
    "        \n",
    "        self.doubleConv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, in_channels, kernel_size=7, padding=3, groups=in_channels), # The sequence length will remain the same\n",
    "            nn.LayerNorm(normalized_shape=[in_channels, seq_length]),\n",
    "            nn.Conv1d(in_channels, mid_channels, kernel_size=7, padding=3),\n",
    "            nn.Conv1d(mid_channels, out_channels, kernel_size=7, padding=3), # The sequence length will remain the same\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.doubleConv(x)\n",
    "\n",
    "    \n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downsampling using maxpooling, followed by 2 layers of convolution\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None, special=False, seq_length=150):\n",
    "        super().__init__()\n",
    "        if not special:\n",
    "            self.maxpoolConv = nn.Sequential(\n",
    "                nn.MaxPool1d(2),\n",
    "                DoubleConv(in_channels, out_channels, mid_channels, seq_length)\n",
    "            )\n",
    "        else:\n",
    "            self.maxpoolConv = nn.Sequential(\n",
    "                nn.MaxPool1d(kernel_size=2, stride=2, padding=1), \n",
    "                DoubleConv(in_channels, out_channels, mid_channels, seq_length)\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.maxpoolConv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upsampling (with skip connection) then 2 conv layers\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None, special=False, seq_length=150):\n",
    "        super().__init__()\n",
    "        \n",
    "        if not special:\n",
    "            self.up = nn.ConvTranspose1d(in_channels, in_channels // 2, kernel_size=2, stride=2) \n",
    "            self.conv = DoubleConv(in_channels, out_channels, mid_channels=mid_channels, seq_length=seq_length)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose1d(in_channels, in_channels // 2, kernel_size=3, stride=2, padding=1) #[256, 38] -> [128, 75]\n",
    "            self.conv = DoubleConv(in_channels, out_channels, mid_channels=mid_channels, seq_length=seq_length) #[256, 38] -> [128, 75]\n",
    "            \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1) # [128, 75]\n",
    "        x = torch.cat([x2, x1], dim=1) # [256, 75]\n",
    "        return self.conv(x)\n",
    "        \n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "    \n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear1 = nn.Linear(in_dim, 64)\n",
    "        self.dropout1 = nn.Dropout()\n",
    "        self.relu1 = nn.GELU()\n",
    "        self.linear2 = nn.Linear(64, out_dim)\n",
    "        self.dropout2 = nn.Dropout()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class Unet1D(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        super(Unet1D, self).__init__()\n",
    "                \n",
    "        self.inConv = DoubleConv(n_channels, 32) # (150+2*1-3)/1+1 = 150 (4 -> 32)\n",
    "        self.selayer1 = SELayer(32)\n",
    "        self.down1 = Down(32, 64, seq_length=75) # (Maxpooling -> 75) (32 -> 64)\n",
    "        self.selayer2 = SELayer(64)\n",
    "        self.down2 = Down(64, 128, special=True, seq_length=38) # (Maxpooling -> 38) (64 -> 128)\n",
    "        self.selayer3 = SELayer(128)\n",
    "        self.down3 = Down(128, 256, seq_length=19) # (Maxpooling -> 19) (128 -> 256)\n",
    "        \n",
    "        self.up1 = Up(256, 128, mid_channels=512, seq_length=38) # (ConvTranspose -> 38) (256 -> 128)\n",
    "        self.up2 = Up(128, 64, mid_channels=256, special=True, seq_length=75) # (ConvTranspose -> 75) (128 -> 64)\n",
    "        self.up3 = Up(64, 32, mid_channels=128) # (ConvTranspose -> 150) (64 -> 32)\n",
    "        self.outConv = OutConv(32, n_classes)\n",
    "        self.classConv = MLP(150, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.inConv(x) # [x, 32, 150]\n",
    "        x1 = self.selayer1(x1)\n",
    "        x2 = self.down1(x1) # [x, 64, 75]\n",
    "        x2 = self.selayer2(x2)\n",
    "        x3 = self.down2(x2) # [x, 128, 38]\n",
    "        x3 = self.selayer3(x3)\n",
    "        x4 = self.down3(x3) # [x, 256, 19]\n",
    "        \n",
    "        x = self.up1(x4, x3) # [x, 128, 38]\n",
    "        x = self.up2(x, x2) # [x, 64, 75]\n",
    "        x = self.up3(x, x1) # [x, 32, 150]\n",
    "        out = self.outConv(x) # [x, 1, 150]\n",
    "        #out = torch.sigmoid(out)\n",
    "        #out_conf = self.classConv(out.view(out.size()[0], -1))\n",
    "        out_conf = self.classConv(out.squeeze(1))\n",
    "        return (torch.sigmoid(out), torch.sigmoid(out_conf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1281ab7d",
   "metadata": {},
   "source": [
    "# 3. Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5f1f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sliding sequences\n",
    "train_seqs, train_labels = sliding(train_sigma70)\n",
    "val_seqs, val_labels = sliding(val_sigma70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b0590c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "train_set = SeqDataset(train_seqs, train_labels, bendability_ref_normed)\n",
    "val_set = SeqDataset(val_seqs, val_labels, bendability_ref_normed)\n",
    "\n",
    "# Create dataloader\n",
    "train_dl = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dl = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe081a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define model\n",
    "unet = Unet1D(5, 1)\n",
    "unet.cuda()\n",
    "optimizer = torch.optim.AdamW(unet.parameters(), lr=0.001)\n",
    "\n",
    "train_loss_his = []\n",
    "val_loss_his = []\n",
    "\n",
    "for epoch in range(1000):\n",
    "    print(\"Epoch {}\".format(epoch))\n",
    "    start = time.time()\n",
    "    train_eloss = 0\n",
    "    val_eloss = 0\n",
    "\n",
    "    unet.train()\n",
    "    for x, x_label in train_dl:\n",
    "        x = x.cuda()\n",
    "        x_label = x_label.cuda()\n",
    "        recon, recon_conf = unet(x)\n",
    "        \n",
    "        focal_loss = criterion(recon, x_label)\n",
    "        \n",
    "        conf = class_loss(recon_conf, torch.sum(x_label, dim = -1))\n",
    "        loss = focal_loss + 0.1 * conf\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_eloss += loss.cpu().item()\n",
    "\n",
    "    unet.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (x, x_label) in enumerate(val_dl):\n",
    "            x = x.cuda()\n",
    "            x_label = x_label.cuda()\n",
    "            recon, recon_conf = unet(x)\n",
    "            val_focal_loss = criterion(recon, x_label) \n",
    "            val_conf = class_loss(recon_conf, torch.sum(x_label, dim = -1))\n",
    "            val_loss = val_focal_loss + 0.1 * val_conf\n",
    "\n",
    "            val_eloss += val_loss.cpu().item()\n",
    "\n",
    "    train_loss_his.append(train_eloss/len(train_dl))\n",
    "    val_loss_his.append(val_eloss/len(val_dl))\n",
    "\n",
    "    if epoch == 0:\n",
    "        loss_val_history = val_loss_his[-1]\n",
    "        patience = 0\n",
    "    else:\n",
    "        loss_val_history = np.append(loss_val_history, val_loss_his[-1])\n",
    "\n",
    "    if val_loss_his[-1] < 0.00000000000000001 + np.min(loss_val_history):\n",
    "        patience = 0\n",
    "        model = \"best_model_BEND_K7.pt\"\n",
    "        torch.save(unet.state_dict(), model)\n",
    "    else:\n",
    "        patience +=1\n",
    "\n",
    "    end = time.time()\n",
    "    hours, rem = divmod(end-start, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "    print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n",
    "    print(epoch, patience, val_eloss/len(val_dl), np.min(loss_val_history))\n",
    "\n",
    "    if patience == 5:\n",
    "        model = \"best_model_BEND_K7.pt\"\n",
    "        unet.load_state_dict(torch.load(model))\n",
    "\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
